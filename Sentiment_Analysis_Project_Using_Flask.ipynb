{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "NPdYzzyz7Rme",
        "outputId": "397b77b2-b527-4355-a77c-c36b2f6676ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import joblib\n",
        "import requests\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request as urllib\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from flask import Flask,render_template,request\n",
        "import time\n",
        "\n",
        "# Flipkart Reviews extraction and sentiment analysis\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0\n",
        "\n",
        "\n",
        "def clean(x):\n",
        "    x = re.sub(r'[^a-zA-Z ]', ' ', x) # replace evrything thats not an alphabet with a space\n",
        "    x = re.sub(r'\\s+', ' ', x) #replace multiple spaces with one space\n",
        "    x = re.sub(r'READ MORE', '', x) # remove READ MORE\n",
        "    x = x.lower()\n",
        "    x = x.split()\n",
        "    y = []\n",
        "    for i in x:\n",
        "        if len(i) >= 3:\n",
        "            if i == 'osm':\n",
        "                y.append('awesome')\n",
        "            elif i == 'nyc':\n",
        "                y.append('nice')\n",
        "            elif i == 'thanku':\n",
        "                y.append('thanks')\n",
        "            elif i == 'superb':\n",
        "                y.append('super')\n",
        "            else:\n",
        "                y.append(i)\n",
        "    return ' '.join(y)\n",
        "\n",
        "\n",
        "def extract_all_reviews(url, clean_reviews, org_reviews,customernames,commentheads,ratings):\n",
        "    with urllib.urlopen(url) as u:\n",
        "        page = u.read()\n",
        "        page_html = BeautifulSoup(page, \"html.parser\")\n",
        "    reviews = page_html.find_all('div', {'class': 't-ZTKy'})\n",
        "    commentheads_ = page_html.find_all('p',{'class':'_2-N8zT'})\n",
        "    customernames_ = page_html.find_all('p',{'class':'_2sc7ZR _2V5EHH'})\n",
        "    ratings_ = page_html.find_all('div',{'class':['_3LWZlK _1BLPMq','_3LWZlK _32lA32 _1BLPMq','_3LWZlK _1rdVr6 _1BLPMq']})\n",
        "\n",
        "    for review in reviews:\n",
        "        x = review.get_text()\n",
        "        org_reviews.append(re.sub(r'READ MORE', '', x))\n",
        "        clean_reviews.append(clean(x))\n",
        "    \n",
        "    for cn in customernames_:\n",
        "        customernames.append('~'+cn.get_text())\n",
        "    \n",
        "    for ch in commentheads_:\n",
        "        commentheads.append(ch.get_text())\n",
        "    \n",
        "    ra = []\n",
        "    for r in ratings_:\n",
        "        try:\n",
        "            if int(r.get_text()) in [1,2,3,4,5]:\n",
        "                ra.append(int(r.get_text()))\n",
        "            else:\n",
        "                ra.append(0)\n",
        "        except:\n",
        "            ra.append(r.get_text())\n",
        "        \n",
        "    ratings += ra\n",
        "    print(ratings)\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('home.html')\n",
        "\n",
        "@app.route('/results',methods=['GET'])\n",
        "def result():    \n",
        "    url = request.args.get('url')\n",
        "\n",
        "    nreviews = int(request.args.get('num'))\n",
        "    clean_reviews = []\n",
        "    org_reviews = []\n",
        "    customernames = []\n",
        "    commentheads = []\n",
        "    ratings = []\n",
        "\n",
        "    with urllib.urlopen(url) as u:\n",
        "        page = u.read()\n",
        "        page_html = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "    proname = page_html.find_all('span', {'class': 'B_NuCI'})[0].get_text()\n",
        "    price = page_html.find_all('div', {'class': '_30jeq3 _16Jk6d'})[0].get_text()\n",
        "    \n",
        "    # getting the link of see all reviews button\n",
        "    all_reviews_url = page_html.find_all('div', {'class': 'col JOpGWq'})[0]\n",
        "    all_reviews_url = all_reviews_url.find_all('a')[-1]\n",
        "    all_reviews_url = 'https://www.flipkart.com'+all_reviews_url.get('href')\n",
        "    url2 = all_reviews_url+'&page=1'\n",
        "    \n",
        "\n",
        "    # start reading reviews and go to next page after all reviews are read \n",
        "    while True:\n",
        "        x = len(clean_reviews)\n",
        "        # extracting the reviews\n",
        "        extract_all_reviews(url2, clean_reviews, org_reviews,customernames,commentheads,ratings)\n",
        "        url2 = url2[:-1]+str(int(url2[-1])+1)\n",
        "        if x == len(clean_reviews) or len(clean_reviews)>=nreviews:break\n",
        "\n",
        "    org_reviews = org_reviews[:nreviews]\n",
        "    clean_reviews = clean_reviews[:nreviews]\n",
        "    customernames = customernames[:nreviews]\n",
        "    commentheads = commentheads[:nreviews]\n",
        "    ratings = ratings[:nreviews]\n",
        "\n",
        "\n",
        "    # building our wordcloud and saving it\n",
        "    for_wc = ' '.join(clean_reviews)\n",
        "    wcstops = set(STOPWORDS)\n",
        "    wc = WordCloud(width=1400,height=800,stopwords=wcstops,background_color='white').generate(for_wc)\n",
        "    plt.figure(figsize=(20,10), facecolor='k', edgecolor='k')\n",
        "    plt.imshow(wc, interpolation='bicubic') \n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    CleanCache(directory='static/images')\n",
        "    plt.savefig('static/images/woc.png')\n",
        "    plt.close()\n",
        "\n",
        "    \n",
        "    # making a dictionary of product attributes and saving all the products in a list\n",
        "    d = []\n",
        "    for i in range(len(org_reviews)):\n",
        "        x = {}\n",
        "        x['review'] = org_reviews[i]\n",
        "        # x['sent'] = predictions[i]\n",
        "        x['cn'] = customernames[i]\n",
        "        x['ch'] = commentheads[i]\n",
        "        x['stars'] = ratings[i]\n",
        "        d.append(x)\n",
        "    \n",
        "\n",
        "    for i in d:\n",
        "        if i['stars']!=0:\n",
        "            if i['stars'] in [1,2]:\n",
        "                i['sent'] = 'NEGATIVE'\n",
        "            else:\n",
        "                i['sent'] = 'POSITIVE'\n",
        "    \n",
        "\n",
        "    np,nn =0,0\n",
        "    for i in d:\n",
        "        if i['sent']=='NEGATIVE':nn+=1\n",
        "        else:np+=1\n",
        "\n",
        "    return render_template('result.html',dic=d,n=len(clean_reviews),nn=nn,np=np,proname=proname,price=price)\n",
        "    \n",
        "    \n",
        "@app.route('/wc')\n",
        "def wc():\n",
        "    return render_template('wc.html')\n",
        "\n",
        "\n",
        "class CleanCache:\n",
        "    '''\n",
        "    this class is responsible to clear any residual csv and image files\n",
        "    present due to the past searches made.\n",
        "    '''\n",
        "    def __init__(self, directory=None):\n",
        "        self.clean_path = directory\n",
        "        # only proceed if directory is not empty\n",
        "        if os.listdir(self.clean_path) != list():\n",
        "            # iterate over the files and remove each file\n",
        "            files = os.listdir(self.clean_path)\n",
        "            for fileName in files:\n",
        "                print(fileName)\n",
        "                os.remove(os.path.join(self.clean_path,fileName))\n",
        "        print(\"cleaned!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n",
        "\n",
        "# this was the code for Flipkart Reviews extraction and sentiment analysis"
      ]
    }
  ]
}